# Enhanced Docker Compose with Local LLM Support
version: '3.8'

services:
  # Main healthcare scraper service
  healthcare-scraper:
    build:
      context: .
      dockerfile: Dockerfile.enhanced
    container_name: healthcare-scraper
    volumes:
      - ./output:/app/output
      - ./input:/app/input
      - ./logs:/app/logs
    environment:
      - PYTHONUNBUFFERED=1
      - PLAYWRIGHT_BROWSERS_PATH=/ms-playwright
      # OpenRouter API key (if using cloud models)
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      # Ollama connection (if using local Ollama service)
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    networks:
      - scraper-network
    profiles:
      - scraper
      - full

  # Local Ollama service for running LLMs locally
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    networks:
      - scraper-network
    profiles:
      - ollama
      - full
    # Optional: GPU support (uncomment if you have NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Text Generation WebUI (alternative local LLM server)
  text-generation-webui:
    image: atinoda/text-generation-webui:default
    container_name: text-generation-webui
    ports:
      - "7860:7860"
    volumes:
      - ./models:/app/models
      - ./characters:/app/characters
      - ./presets:/app/presets
    environment:
      - CLI_ARGS=--api --listen --listen-host 0.0.0.0 --listen-port 7860
    networks:
      - scraper-network
    profiles:
      - webui
      - full
    # Uncomment for GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Nginx for serving results
  nginx:
    image: nginx:alpine
    container_name: scraper-nginx
    ports:
      - "8080:80"
    volumes:
      - ./output:/usr/share/nginx/html/results:ro
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    networks:
      - scraper-network
    profiles:
      - web
      - full

  # Model downloader service (runs once to download models)
  model-downloader:
    build:
      context: .
      dockerfile: Dockerfile.enhanced
    container_name: model-downloader
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    networks:
      - scraper-network
    profiles:
      - setup
    command: >
      sh -c "
        echo 'Waiting for Ollama to start...' &&
        sleep 10 &&
        curl -X POST http://ollama:11434/api/pull -d '{\"name\":\"llama3.2:3b\"}' &&
        curl -X POST http://ollama:11434/api/pull -d '{\"name\":\"phi3:mini\"}' &&
        echo 'Models downloaded successfully'
      "

volumes:
  ollama-data:
    driver: local

networks:
  scraper-network:
    driver: bridge

